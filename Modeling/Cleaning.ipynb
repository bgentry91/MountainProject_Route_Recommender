{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T21:09:01.693240Z",
     "start_time": "2018-02-23T21:09:01.645730Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Scraped Data from MountainProject.com\n",
    "\n",
    "Importing data froms scraped files - you can use your own filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T21:09:01.816921Z",
     "start_time": "2018-02-23T21:09:01.808450Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix_to_import = ['a-i','ca','co','gunks', 'k-nj', 'nm-s',\n",
    "                   't-v','w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T21:09:05.017714Z",
     "start_time": "2018-02-23T21:09:01.977449Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = None\n",
    "for prefix in prefix_to_import:\n",
    "    if df is None:\n",
    "        df = pd.read_csv('Text_Data/' + prefix + '_routes_text.csv')\n",
    "    else:\n",
    "        df2 = pd.read_csv('Text_Data/' + prefix + '_routes_text.csv')\n",
    "        df = pd.concat([df,df2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T21:09:06.221800Z",
     "start_time": "2018-02-23T21:09:05.020456Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_d = None\n",
    "for prefix in prefix_to_import:\n",
    "    if df_d is None:\n",
    "        df_d = pd.read_csv('Scraped_Data/' + prefix + '_route_data.csv')\n",
    "    else:\n",
    "        df_d2 = pd.read_csv('Scraped_Data/' + prefix + '_route_data.csv')\n",
    "        df_d = pd.concat([df_d,df_d2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T21:09:06.591671Z",
     "start_time": "2018-02-23T21:09:06.225379Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_url = None\n",
    "for prefix in prefix_to_import:\n",
    "    if df_url is None:\n",
    "        df_url = pd.read_csv('Route_URL/' + prefix + '_routes.csv')\n",
    "    else:\n",
    "        df_url2 = pd.read_csv('Route_URL/' + prefix + '_routes.csv')\n",
    "        df_url = pd.concat([df_url,df_url2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up .csv nonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T21:09:06.601903Z",
     "start_time": "2018-02-23T21:09:06.594470Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_url['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T21:09:06.616730Z",
     "start_time": "2018-02-23T21:09:06.604980Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_d['Unnamed: 0']\n",
    "del df_d['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T21:11:44.899379Z",
     "start_time": "2018-02-23T21:09:06.620955Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.groupby('id',as_index=False).first()\n",
    "df_d = df_d.groupby('id',as_index=False).first()\n",
    "df_url = df_url.groupby('id',as_index=False).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine urls & route data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T21:11:45.083369Z",
     "start_time": "2018-02-23T21:11:44.902824Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_d = df_d.merge(df_url, on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up rating to follow standard format (5.XXa)\n",
    "* Some fun with Regex - catching all of the scenarios is a little tricky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T21:11:45.102951Z",
     "start_time": "2018-02-23T21:11:45.087471Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_rating(messy_rating):\n",
    "    import re\n",
    "    match = re.search('5\\.[0-9]+[a-z]', messy_rating)\n",
    "    if match:\n",
    "        rate = match.group(0)\n",
    "        if len(rate) == 3:\n",
    "            return rate[:2] + '0' + rate[2]\n",
    "        else:\n",
    "            return rate\n",
    "    else:\n",
    "        match = re.search('5\\.[0-9]+', messy_rating)\n",
    "        if match:\n",
    "            rate = match.group(0)\n",
    "            if len(rate) == 3:\n",
    "                return rate[:2] + '0' + rate[2]\n",
    "            else:\n",
    "                return rate\n",
    "        else:\n",
    "            return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T21:11:48.133843Z",
     "start_time": "2018-02-23T21:11:45.110632Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_d['rating'] = df_d.apply(lambda x: clean_rating(x['rating']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T21:11:48.505587Z",
     "start_time": "2018-02-23T21:11:48.145716Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('full_dataset.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(df_d, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up text data\n",
    "* Remove /n\n",
    "* Remove leading/ending spaces\n",
    "* Combine descriptions and comments for one long document (not used in final model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T18:30:56.211066Z",
     "start_time": "2018-02-22T18:30:17.544527Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_cols = ['Comment', 'Loc', 'Desc', 'Intro', 'Prot', 'Descent', 'Escape']\n",
    "for col in text_cols:\n",
    "    df[col] = df.apply(lambda x: str(x[col]).replace('\\n',''), axis=1)\n",
    "    df[col] = df.apply(lambda x: str(x[col]).strip(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T18:30:56.554416Z",
     "start_time": "2018-02-22T18:30:56.214942Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Desc_Comment'] = df['Desc'] + df['Comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenize each document for improved accuracy\n",
    "* Need to keep track of routes associated with each sentence\n",
    "* Use dictionary to build this, then convert to DF for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T18:01:55.288216Z",
     "start_time": "2018-02-23T18:01:11.127879Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = df.apply(lambda x: sent_tokenize(x['Desc']), axis = 1).tolist()\n",
    "id_list = df['id'].tolist()\n",
    "\n",
    "sentence_list = []\n",
    "id_num = []\n",
    "for i, rec in enumerate(x):\n",
    "    for sent in rec:\n",
    "        if 'nan' not in sent:\n",
    "            sentence_list.append(sent)\n",
    "            id_num.append(id_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T18:01:55.864793Z",
     "start_time": "2018-02-23T18:01:55.291306Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_dict = {}\n",
    "sent_dict['id'] = id_num\n",
    "sent_dict['sentence'] = sentence_list\n",
    "\n",
    "df_sent = pd.DataFrame.from_dict(sent_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training dataset using sampled routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T18:30:56.743503Z",
     "start_time": "2018-02-22T18:30:56.559124Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_ids = df.sample(20000)['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T18:01:55.985877Z",
     "start_time": "2018-02-23T18:01:55.869248Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_sent[df_sent['id'].isin(sample_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output data for use in final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T18:01:56.175496Z",
     "start_time": "2018-02-23T18:01:55.992904Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('df_train.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(df_train, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T18:01:56.954948Z",
     "start_time": "2018-02-23T18:01:56.179017Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('df_sent.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(df_sent, picklefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
